---
title: "Accessing Spark from R"
author: Joseph Rickert
date: 06/25/18
output: html_notebook
---

This notebook describes how users can access  [`Spark`](http://spark.apache.org/) from R and make use of Spark resources through the [`sparklyr`](https://CRAN.R-project.org/package=sparklyr) package.

The example presented is adapted from the  [Online Documentation](https://spark.rstudio.com).


With `sparklyr` R users can: 
* Use `dplyr` functions to access `Spark`
* Filter and aggregate `Spark` datasets and them bring them into R for analysis and visualization    
* Use `Spark's` distributed machine learning library [MLlib](http://spark.apache.org/docs/latest/mllib-guide.html)  
* Create [extensions](https://spark.rstudio.com/extensions.html) that call the full `Spark` API and provide interfaces to `Spark` packages

Another way to think of things is that `sparklyr` allows `Spark` to act as a backend for `dplyr` and other R packages and become part of the data science workflow in R.

![](workflow.png)

### Preliminaries
In this section, we assume that that the `sparklyr` package has been installed (`install.packages("sparklyr")`). We use the function `sparklyr::spark_install()` to install a local copy of `Spark`. 

```{r, eval=FALSE, message=FALSE, warning=FALSE}
library(sparklyr)
spark_install(version = "2.1.0")
```

First let's get a feel for what's in `sparklyr` by looking at the on-line documentation.
```{r}
help(package="sparklyr")
#ls('package:sparklyr')
```

### Connecting to Spark

There are two ways to use `sparklyr` with `Spark`. We call these two **deployment** modes:  
* Local - running `Spark` locally on the desktop   
* Cluster - working directly within or alongside a Spark cluster ([standalone](http://spark.apache.org/docs/latest/spark-standalone.html), [YARN](http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html), [Mesos](http://mesos.apache.org/), etc.)

For this example, we will use a local connection which can be established either by using the function `sparklyr::spark_connect()` or through the Connection window in the RStudio IDE. For information on making cluster connections see the [Deployment and Configuration](https://spark.rstudio.com/articles/deployment-overview.html) overview.

```{r, message=FALSE}
library(sparklyr)
sc <- spark_connect(master = "local")
```

### Moving Data into Spark
We begin by copying the some data sets into `Spark`. We will use the `nycflights13` data, the `Lahman` dataset containing baseball stats, and good old `iris`.

```{r, eval=FALSE}
#install.packages(c("nycflights13", "Lahman"))
```

```{r, message=FALSE}
library(dplyr)
library(nycflights13)
library(Lahman)
iris_tbl <- copy_to(sc, iris)
flights_tbl <- copy_to(sc, nycflights13::flights, "flights")
batting_tbl <- copy_to(sc, Lahman::Batting, "batting")
```

Now, we check to see what tables we have in Spark. Notice that the tables also show up in the RStudio IDE Connection pane.

```{r}
src_tbls(sc)
```

### Using `dplyr` with data in `Spark`

In this first example, we use `dplyr::filter()` to fetch flights with two minute departure delays.

```{r}
# filter by departure delay and print the first few records
flights_tbl %>% filter(dep_delay == 2)
```

Here, we compute mean distance and mean arrival delay for flights with more than 20 observations and distances less than 2,000 miles.

```{r}
delay <- flights_tbl %>% 
  group_by(tailnum) %>%
  summarise(count = n(), dist = mean(distance), delay = mean(arr_delay)) %>%
  filter(count > 20, dist < 2000, !is.na(delay)) %>%
  collect
``` 

Then, we plot that information.    

```{r}

# plot delays
library(ggplot2)
ggplot(delay, aes(dist, delay)) +
  geom_point(aes(size = count), alpha = 1/2) +
  geom_smooth() +
  scale_size_area(max_size = 2)
```    

In a slightly more elaborate example, we show a typical dplyr workflow using the baseball data.

```{r}
batting_tbl
```

We select some of the columns of from `batting-tbl` sort by player, year and team, group by players then rank the data by hits and select the top two years for each player in which he had at least one hit. 


```{r}
batting_tbl %>%
  select(playerID, yearID, teamID, G, AB:H) %>%
  arrange(playerID, yearID, teamID) %>%
  group_by(playerID) %>%
  filter(min_rank(desc(H)) <= 2 & H > 0)
```

### Using SQL
We can also use SQL queries to interrogate tables in a `Spark` cluster. The `spark_connection` object implements a DBI interface for Spark. We can use `dbGetQuery` to execute SQL and return the result as an R data frame. We could have used a SQL query to look at just the first 10 rows of the baseball data set.

```{r}
library(DBI)
batting_preview <- dbGetQuery(sc, "SELECT * FROM batting LIMIT 10")
batting_preview
```   


### A Regression Example

Spark's `MLlib` [machine learning library](http://spark.apache.org/docs/latest/mllib-guide.html) can be accessed directly from `sparklyr` through functions that connect to a high-level API's and make use of Spark `DataFrames`. Here is a very simple example that uses `sparklyr`'s `ml_linear_regression()` function to fit a linear regression to data in Spark.

First, we partition the data into training and test data sets, as you might do with a real problem. Notice that one of the predictor variables `carrier` is of type character. `sparklyr` is going to take care of the one hot encoding to make this a categorical variable (factor in R) on the fly. Also note that we are filtering out missing values, because they cause SPARK problems.

```{r}
partitions <- flights_tbl %>%
  filter(!is.na(arr_delay)) %>%
  sdf_partition(training = 0.9, test = 0.5, seed = 1951)
```

The `keras` function `ml_linear_regression()` initiates the `MLlib` regression algorithm.

```{r}
fit <- partitions$training %>%
  ml_linear_regression(response = "arr_delay", features = c("dep_time", "distance", "month", "carrier"))

```

`summary()` has a method to extract the model coefficients returned from the `MLlib` model.

```{r}
summary(fit)
```

Next, we use `sdf_predict()` to make predictions with the test set.

```{r}
sdf_predict(partitions$test,fit)
```


Finally, we disconnect from `Spark`.

```{r}
spark_disconnect(sc)
```


### More Information

The main source for sparklyr information is [spark.rstudio.com](https://spark.rstudio.com/)

Look [here](https://github.com/rstudio/webinars/blob/master/42-Introduction%20to%20sparklyr/sparklyr-webinar1.Rmd) for a very nice `sparklr` / `MLlib` logistic regression from a [webinar](https://www.rstudio.com/resources/webinars/introducing-an-r-interface-for-apache-spark/) by Edgar Ruiz.

For more general information, have a look at: the [Gallery] (https://spark.rstudio.com/articles/gallery.html) of `sparklyr` examples, the [Online Documentation](https://spark.rstudio.com) for `sparklyr` and the [Documentation]((https://spark.rstudio.com/examples-emr.html)) for using `sparklyr` with Apache Spark Cluster.










